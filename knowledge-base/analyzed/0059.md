# Finding 0059 Analysis

## Entries

### SS-UNIFIED-STORAGE

**What happens:** All persistent state — granfilade (content store), spanfilade (link index), and POOM enfilades (document structure) — resides in a single disk file `enf.enf`. The file has a fixed layout: header blocks containing a bitmap and metadata, then fixed locations for the granfilade root (`GRANFDISKLOCATION`) and spanfilade root (`SPANFDISKLOCATION`), followed by dynamically allocated blocks (loaves). Each block is `NUMBYTESINLOAF` bytes (typically 1024). Blocks are typed by `denftype` field (GRAN, SPAN, or POOM) in their header, but share the same allocator and on-disk format.

**Block header structure:**
```c
typedef struct structdiskloafhedr {
    INT sizeofthisloaf;
    SINT isapex;           // TRUE if top of orgl
    SINT height;           // 0 = bottom crum (leaf)
    SINT denftype;         // GRAN, SPAN, or POOM
    SINT numberofcrums;
    SINT refcount;         // For subtree sharing / GC
    SINT allignmentdummy;
} typediskloafhedr;
```

Leaf nodes for text content (`GRANTEXT` type) hold up to 950 bytes (`GRANTEXTLENGTH`). Multiple sub-loaves can be packed into a single uber-loaf block.

**Why it matters for spec:** The unified storage means the state space is a single allocation domain. There is no physical isolation between the granfilade and spanfilade — a corrupted block allocator affects all enfilades. For specification, the persistent state is: `Disk = {header, granf_root, spanf_root, blocks: BlockNum → Loaf}` where `Loaf` is tagged by `denftype`. The permascroll (σ : I → B) is implemented as a B-tree of crums, not a flat byte array — retrieval at I-address X requires tree traversal, not direct lookup.

**Code references:**
- `backend/disk.c:364-382` — open/create `enf.enf`
- `backend/coredisk.h:117-120` — `GRANFDISKLOCATION`, `SPANFDISKLOCATION` fixed locations
- `backend/coredisk.h:11-21` — `typediskloafhedr` structure
- `backend/coredisk.h:66-71` — `typeuberdiskloaf` (uber-loaf packing)

**Concrete example:**
```
enf.enf layout:
  Block 0..N:    Disk header (bitmap + metadata)
  Block N+1:     granf root (fixed at GRANFDISKLOCATION)
  Block N+2:     spanf root (fixed at SPANFDISKLOCATION)
  Block N+3...:  Allocated loaves — mix of GRAN, SPAN, POOM blocks
```

**Provenance:** Finding 0059

### SS-CACHE-MECHANISM

**What happens:** All in-memory crums (GRAN, SPAN, POOM) participate in a single shared cache managed by the `grimreaper` global pointer. This forms a circular doubly-linked list of all crums currently in memory. There is no separate write-back buffer — the in-memory enfilade tree IS the cache. Each crum has an `age` counter and `modified` flag. When memory allocation fails, the grim reaper scans for crums with `age >= OLD` and not `RESERVED`, writes modified ones to disk via `orglwrite()`, then frees them.

**Why it matters for spec:** The cache is a shared resource across all enfilades. Memory pressure from one subsystem (e.g., a large link search loading many spanfilade crums) can evict modified crums from another subsystem (e.g., recently inserted text atoms). For spec purposes, the observable state includes both in-memory and on-disk crums. The invariant is: for any crum C, either C is in the grim reaper list (in-memory, possibly modified) or C is on disk at its assigned block number, but never neither (that would be data loss).

**Code references:**
- `backend/credel.c:15` — `typecorecrum *grimreaper` global
- `backend/credel.c:518-532` — `createcrum` adds new crum to circular list
- `backend/credel.c:54-76` — `ealloc` triggers `grimlyreap()` on allocation failure
- `backend/credel.c:106-162` — grim reaper eviction: age-based scan, write modified, free

**Provenance:** Finding 0059

### ST-INSERT

**What happens:** On INSERT, text is copied into an in-memory bottom crum (`typecbc`), and the crum plus its ancestors are marked `modified = TRUE` via `ivemodified()`. The text is NOT immediately written to disk. The crum remains in the grim reaper cache until evicted by memory pressure or flushed at session exit.

**Why it matters for spec:** The postcondition of INSERT includes the text being retrievable (via the in-memory cache), but does NOT include the text being durable on disk. This is a weaker postcondition than a specification might assume. Formally: after INSERT, `RETRIEVE(addr)` returns the text (from cache), but `crash(); restart(); RETRIEVE(addr)` may fail.

**Code references:**
- `backend/insert.c:17-70` — `insertseq` creates leaf crum with text
- `backend/genf.c:522-544` — `ivemodified` marks crum and ancestors as modified

**Provenance:** Finding 0059

### INV-DURABILITY-BOUNDARY

**What happens:** Durability guarantees depend on session lifecycle, not on individual operations:

1. **On clean session exit:** `writeenfilades()` recursively writes all modified crums from both granfilade and spanfilade to disk. This is called from `bed.c:134` during daemon shutdown.
2. **On crash/kill:** Only crums previously evicted by the grim reaper survive. Recent INSERTs still in cache are lost.
3. **No fsync:** `write()` syscalls go to OS buffers; no explicit `fsync()` guarantees.
4. **No transaction log:** Within-session consistency comes from the in-memory cache, not from disk state.

**Why it matters for spec:** A formal specification must distinguish between "operation completed" (in-memory postcondition holds) and "operation is durable" (survives crash). The system provides session-level durability (all-or-nothing at session boundary), not operation-level durability. This is the key durability invariant: `writeenfilades() → ∀ modified crums c: c is on disk`. But absent `writeenfilades()`, durability is best-effort via grim reaper eviction.

**Code references:**
- `backend/corediskout.c:68-88` — `writeenfilades()` writes granf and spanf roots
- `backend/bed.c:134,183` — daemon exit calls `writeenfilades(); closediskfile()`
- `backend/disk.c:300-338` — `actuallywriteloaf` does synchronous `write()` with no `fsync`

**Concrete example:**
```
Session timeline:
  t0: INSERT("hello") → crum in RAM, modified=TRUE
  t1: RETRIEVE → "hello" (from cache) ✓
  t2: [crash]
  t3: restart, RETRIEVE → fails (crum never written to disk)

vs.

  t0: INSERT("hello") → crum in RAM, modified=TRUE
  t1: RETRIEVE → "hello" (from cache) ✓
  t2: clean exit → writeenfilades() flushes to disk
  t3: restart, RETRIEVE → "hello" ✓
```

**Provenance:** Finding 0059

### EC-CRASH-MID-WRITE

**What happens:** The `subtreewriterecurs` function writes modified subtrees bottom-up: children first, then parent. If a crash occurs mid-write (e.g., after writing leaf and middle nodes but before updating the root), the on-disk enfilade enters an inconsistent state. The root pointer still references the old subtree, while newly written child nodes are orphaned — allocated on disk but unreachable from any root. The old tree may reference deallocated blocks that were reassigned to the new children.

Similarly, `writeenfilades()` writes granfilade root then spanfilade root sequentially. A crash between these two writes leaves the granfilade updated but the spanfilade stale — violating the cross-enfilade consistency invariant (every I-address in the granfilade should have a corresponding DOCISPAN in the spanfilade).

**Why it matters for spec:** This defines a corruption class that a specification must acknowledge: the system has no atomicity guarantee for disk writes spanning multiple blocks. The bottom-up write order is a partial mitigation (children exist before parent references them), but without an atomic commit mechanism, no write ordering prevents all corruption scenarios. A crash-safety specification would need: `atomic_write(subtree) → consistent_on_disk(subtree)`, which this system does not provide.

**Code references:**
- `backend/corediskout.c:426-494` — `subtreewriterecurs` bottom-up write with `modified = FALSE` after each child
- `backend/corediskout.c:68-88` — `writeenfilades` writes granf root then spanf root sequentially
- `backend/disk.c:300-338` — `actuallywriteloaf` per-block write with no fsync

**Concrete example:**
```
Modified granfilade subtree (3 levels):
  Root (modified=TRUE)
  ├── Middle (modified=TRUE)
  │   ├── Leaf-A (modified=TRUE)
  │   └── Leaf-B (modified=TRUE)

subtreewriterecurs writes bottom-up:
  Step 1: Write Leaf-A to disk block 47, clear modified  ✓
  Step 2: Write Leaf-B to disk block 48, clear modified  ✓
  Step 3: Write Middle to disk block 49 (refs blocks 47,48)  ✓
  Step 4: Write Root to disk block 12 (refs block 49)  ← CRASH

On restart:
  - Root on disk (block 12) still points to OLD middle (e.g., block 30)
  - Blocks 47, 48, 49 are allocated but unreachable (orphaned)
  - Block 30 may reference deallocated leaf blocks → corruption
```

**Provenance:** Finding 0059

### EC-NO-STARTUP-VALIDATION

**What happens:** On startup, `initenffile()` opens `enf.enf` and reads the block allocation table via `readallocinfo()`. There is no consistency check: no enfilade tree traversal, no checksum verification, no detection of partial writes or orphaned blocks. If the file contains corrupted data from a crash, the backend loads it as-is. Subsequent operations may then fail with `gerror()` (process abort) or silently return corrupt data depending on which blocks are affected.

**Why it matters for spec:** The system has no precondition check on startup that the persistent state is well-formed. A specification that models restart must note: `restart(disk_state) → loaded_state` assumes `well_formed(disk_state)`, but this assumption is not validated. This means post-crash behavior is undefined — the system does not distinguish between "clean prior shutdown" and "crash-corrupted state".

**Code references:**
- `backend/disk.c:340-383` — `initenffile` opens file, reads allocation info, no validation
- `backend/disk.c:364-382` — fallthrough: if file exists, assume it's valid

**Provenance:** Finding 0059

### EC-CROSS-ENFILADE-EVICTION

**What happens:** Because all enfilades share a single cache (grim reaper list), memory pressure from operations on one enfilade can evict modified crums from another. For example, a large link search loading many spanfilade nodes could cause eviction of recently-inserted but not-yet-written granfilade text atoms. The grim reaper selects victims by age, not by enfilade type.

**Why it matters for spec:** This creates a subtle interaction between subsystems. The order in which crums are evicted (and thus written to disk) is determined by access patterns across all enfilades, not by any per-enfilade policy. In crash scenarios, this means durability of content depends on unrelated link operations — a cross-subsystem dependency that a specification should acknowledge.

**Code references:**
- `backend/credel.c:106-162` — grim reaper scans entire circular list regardless of `denftype`
- `backend/credel.c:147` — `isreapable` checks age/modified but not enfilade type

**Provenance:** Finding 0059
